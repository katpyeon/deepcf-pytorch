{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Sampling Utility\n",
        "\n",
        "\uc774 \ub178\ud2b8\ubd81\uc740 \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc0d8\ud50c \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n",
        "\n",
        "**\ubaa9\uc801:**\n",
        "- \ube60\ub978 \ud504\ub85c\ud1a0\ud0c0\uc774\ud551 \ubc0f \ud14c\uc2a4\ud2b8\n",
        "- DMF, MLP, CFNet \ubaa8\ub450 \ub3d9\uc77c\ud55c \uc0d8\ud50c \ub370\uc774\ud130 \uc0ac\uc6a9\n",
        "- \uacf5\uc815\ud55c \uc131\ub2a5 \ube44\uad50\n",
        "\n",
        "**\uc0dd\uc131 \ud30c\uc77c:**\n",
        "- `{dataset}-sample{N}.train.rating`\n",
        "- `{dataset}-sample{N}.test.rating`\n",
        "- `{dataset}-sample{N}.test.negative`\n",
        "\n",
        "**\uc0d8\ud50c\ub9c1 \uc804\ub7b5:**\n",
        "1. \uc804\uccb4 \uc720\uc800 \uc911 \ub79c\ub364\ud558\uac8c N\uba85 \uc120\ud0dd\n",
        "2. \uc120\ud0dd\ub41c \uc720\uc800\uc758 \ubaa8\ub4e0 \uc0c1\ud638\uc791\uc6a9 \ud3ec\ud568\n",
        "3. Test set\ub3c4 \ub3d9\uc77c \uc720\uc800\ub9cc \uc720\uc9c0\n",
        "4. User/Item ID \uc7ac\ub9e4\ud551 (0\ubd80\ud130 \uc5f0\uc18d\uc801\uc73c\ub85c)\n",
        "5. Negative samples\ub294 \uc6d0\ubcf8 \ub370\uc774\ud130 \ud615\uc2dd \uc720\uc9c0 (99\uac1c/\ud14c\uc2a4\ud2b8 \ucf00\uc774\uc2a4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. \uc124\uc815 (Configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from time import time\n",
        "\n",
        "# ============================================================\n",
        "# \ub370\uc774\ud130\uc14b \uc124\uc815\n",
        "# ============================================================\n",
        "DATA_PATH = 'datasets/'\n",
        "DATASET = 'ml-1m'  # \uc6d0\ubcf8 \ub370\uc774\ud130\uc14b \uc774\ub984\n",
        "\n",
        "# ============================================================\n",
        "# \uc0d8\ud50c\ub9c1 \uc124\uc815\n",
        "# ============================================================\n",
        "SAMPLE_SIZES = [100, 500, 1000]  # \uc0dd\uc131\ud560 \uc0d8\ud50c \ud06c\uae30 \ub9ac\uc2a4\ud2b8\n",
        "SEED = 42  # \uc7ac\ud604\uc131\uc744 \uc704\ud55c \ub79c\ub364 \uc2dc\ub4dc\n",
        "\n",
        "# ============================================================\n",
        "# \uae30\ud0c0 \uc124\uc815\n",
        "# ============================================================\n",
        "OVERWRITE = False  # True: \uae30\uc874 \ud30c\uc77c \ub36e\uc5b4\uc4f0\uae30, False: \uc874\uc7ac\ud558\uba74 \uc2a4\ud0b5\n",
        "\n",
        "# \ub79c\ub364 \uc2dc\ub4dc \uc124\uc815\n",
        "random.seed(SEED)\n",
        "\n",
        "print(f\"Dataset: {DATASET}\")\n",
        "print(f\"Sample sizes: {SAMPLE_SIZES}\")\n",
        "print(f\"Random seed: {SEED}\")\n",
        "print(f\"Overwrite existing: {OVERWRITE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. \uc0d8\ud50c\ub9c1 \ud568\uc218 \uc815\uc758"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sample(dataset, sample_users, seed, data_path='datasets/', overwrite=False):\n",
        "    \"\"\"\n",
        "    Create sample dataset from original data.\n",
        "    \n",
        "    Args:\n",
        "        dataset (str): Dataset name (e.g., 'ml-1m')\n",
        "        sample_users (int): Number of users to sample\n",
        "        seed (int): Random seed\n",
        "        data_path (str): Path to datasets directory\n",
        "        overwrite (bool): If True, overwrite existing files\n",
        "    \n",
        "    Returns:\n",
        "        dict: Statistics (num_users, num_items, num_train, num_test)\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    random.seed(seed)\n",
        "    \n",
        "    # Sample file paths\n",
        "    sample_train = f\"{data_path}{dataset}-sample{sample_users}.train.rating\"\n",
        "    sample_test = f\"{data_path}{dataset}-sample{sample_users}.test.rating\"\n",
        "    sample_neg = f\"{data_path}{dataset}-sample{sample_users}.test.negative\"\n",
        "    \n",
        "    # Check if files already exist\n",
        "    if not overwrite and os.path.exists(sample_train) and os.path.exists(sample_test) and os.path.exists(sample_neg):\n",
        "        print(f\"  \u2713 Sample files already exist (sample{sample_users}). Skipping.\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\nCreating sample data with {sample_users} users...\")\n",
        "    t_start = time()\n",
        "    \n",
        "    # Original file paths\n",
        "    orig_train = f\"{data_path}{dataset}.train.rating\"\n",
        "    orig_test = f\"{data_path}{dataset}.test.rating\"\n",
        "    orig_neg = f\"{data_path}{dataset}.test.negative\"\n",
        "    \n",
        "    # 1. Read train file and extract all users\n",
        "    print(\"  [1/6] Reading train file...\")\n",
        "    all_users = set()\n",
        "    train_data = []\n",
        "    with open(orig_train, 'r') as f:\n",
        "        for line in f:\n",
        "            arr = line.strip().split('\\t')\n",
        "            user, item, rating, timestamp = int(arr[0]), int(arr[1]), arr[2], arr[3]\n",
        "            all_users.add(user)\n",
        "            train_data.append((user, item, rating, timestamp))\n",
        "    \n",
        "    print(f\"        Found {len(all_users)} total users, {len(train_data)} interactions\")\n",
        "    \n",
        "    # 2. Sample SAMPLE_USERS randomly\n",
        "    print(f\"  [2/6] Sampling {sample_users} users...\")\n",
        "    sampled_users = set(random.sample(sorted(all_users), min(sample_users, len(all_users))))\n",
        "    \n",
        "    # 3. Filter train data and remap IDs\n",
        "    print(\"  [3/6] Filtering and remapping IDs...\")\n",
        "    sampled_train = [(u, i, r, t) for u, i, r, t in train_data if u in sampled_users]\n",
        "    \n",
        "    # Extract unique users/items\n",
        "    unique_users = sorted(set(u for u, _, _, _ in sampled_train))\n",
        "    unique_items = sorted(set(i for _, i, _, _ in sampled_train))\n",
        "    \n",
        "    # Create ID mappings\n",
        "    user_map = {old_id: new_id for new_id, old_id in enumerate(unique_users)}\n",
        "    item_map = {old_id: new_id for new_id, old_id in enumerate(unique_items)}\n",
        "    \n",
        "    print(f\"        Sampled: {len(unique_users)} users, {len(unique_items)} items, {len(sampled_train)} interactions\")\n",
        "    \n",
        "    # 4. Write train file\n",
        "    print(\"  [4/6] Writing sample train file...\")\n",
        "    with open(sample_train, 'w') as f:\n",
        "        for u, i, r, t in sampled_train:\n",
        "            f.write(f\"{user_map[u]}\\t{item_map[i]}\\t{r}\\t{t}\\n\")\n",
        "    \n",
        "    # 5. Filter and write test file\n",
        "    print(\"  [5/6] Writing sample test files...\")\n",
        "    test_data = []\n",
        "    with open(orig_test, 'r') as f:\n",
        "        for line in f:\n",
        "            arr = line.strip().split('\\t')\n",
        "            user, item = int(arr[0]), int(arr[1])\n",
        "            if user in sampled_users and item in item_map:\n",
        "                test_data.append((user, item, arr[2], arr[3]))\n",
        "    \n",
        "    with open(sample_test, 'w') as f:\n",
        "        for u, i, r, t in test_data:\n",
        "            f.write(f\"{user_map[u]}\\t{item_map[i]}\\t{r}\\t{t}\\n\")\n",
        "    \n",
        "    # 6. Write test negative file\n",
        "    print(\"  [6/6] Writing sample test negative file...\")\n",
        "    \n",
        "    # Read original negative file as dictionary\n",
        "    orig_neg_dict = {}\n",
        "    with open(orig_neg, 'r') as f:\n",
        "        for line in f:\n",
        "            arr = line.strip().split('\\t')\n",
        "            user_item_pair = arr[0]\n",
        "            user = int(user_item_pair.split(',')[0][1:])\n",
        "            item = int(user_item_pair.split(',')[1][:-1])\n",
        "            orig_neg_dict[(user, item)] = arr[1:]\n",
        "    \n",
        "    # Write negative file in test_data order\n",
        "    with open(sample_neg, 'w') as f_out:\n",
        "        for u_orig, i_orig, _, _ in test_data:\n",
        "            if (u_orig, i_orig) in orig_neg_dict:\n",
        "                orig_neg_items = orig_neg_dict[(u_orig, i_orig)]\n",
        "                \n",
        "                # Filter negatives (only items in item_map)\n",
        "                neg_items = [int(x) for x in orig_neg_items if int(x) in item_map]\n",
        "                neg_items_remapped = [item_map[i] for i in neg_items]\n",
        "                \n",
        "                # Ensure 99 negatives\n",
        "                if len(neg_items_remapped) >= 99:\n",
        "                    neg_items_remapped = neg_items_remapped[:99]\n",
        "                else:\n",
        "                    # Fill with random items if not enough\n",
        "                    available_items = list(item_map.values())\n",
        "                    while len(neg_items_remapped) < 99:\n",
        "                        rand_item = random.choice(available_items)\n",
        "                        if rand_item not in neg_items_remapped:\n",
        "                            neg_items_remapped.append(rand_item)\n",
        "                \n",
        "                # Write remapped user/item IDs\n",
        "                u_new = user_map[u_orig]\n",
        "                i_new = item_map[i_orig]\n",
        "                f_out.write(f\"({u_new},{i_new})\\t\" + '\\t'.join(map(str, neg_items_remapped)) + '\\n')\n",
        "            else:\n",
        "                # Generate random negatives if not found\n",
        "                print(f\"        Warning: No negatives found for user {u_orig}, item {i_orig}. Generating random.\")\n",
        "                available_items = list(item_map.values())\n",
        "                neg_items_remapped = random.sample(available_items, min(99, len(available_items)))\n",
        "                u_new = user_map[u_orig]\n",
        "                i_new = item_map[i_orig]\n",
        "                f_out.write(f\"({u_new},{i_new})\\t\" + '\\t'.join(map(str, neg_items_remapped)) + '\\n')\n",
        "    \n",
        "    t_end = time()\n",
        "    print(f\"\\n  \u2713 Sample files created in {t_end - t_start:.1f}s\")\n",
        "    print(f\"    - {sample_train}\")\n",
        "    print(f\"    - {sample_test}\")\n",
        "    print(f\"    - {sample_neg}\")\n",
        "    \n",
        "    return {\n",
        "        'num_users': len(unique_users),\n",
        "        'num_items': len(unique_items),\n",
        "        'num_train': len(sampled_train),\n",
        "        'num_test': len(test_data)\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"\u2713 Sampling function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. \uc0d8\ud50c \ub370\uc774\ud130 \uc0dd\uc131"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SAMPLE DATA GENERATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for sample_size in SAMPLE_SIZES:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Generating sample: {DATASET}-sample{sample_size}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    stats = create_sample(\n",
        "        dataset=DATASET,\n",
        "        sample_users=sample_size,\n",
        "        seed=SEED,\n",
        "        data_path=DATA_PATH,\n",
        "        overwrite=OVERWRITE\n",
        "    )\n",
        "    \n",
        "    if stats:\n",
        "        results[sample_size] = stats\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"GENERATION COMPLETE\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. \uac80\uc99d \ubc0f \uc694\uc57d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"\\nSample Data Summary:\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'Sample':<20} {'Users':<10} {'Items':<10} {'Train':<15} {'Test':<10} {'File Sizes':<30}\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for sample_size in SAMPLE_SIZES:\n",
        "    sample_name = f\"{DATASET}-sample{sample_size}\"\n",
        "    train_file = f\"{DATA_PATH}{sample_name}.train.rating\"\n",
        "    test_file = f\"{DATA_PATH}{sample_name}.test.rating\"\n",
        "    neg_file = f\"{DATA_PATH}{sample_name}.test.negative\"\n",
        "    \n",
        "    if os.path.exists(train_file):\n",
        "        # Get file sizes\n",
        "        train_size = os.path.getsize(train_file) / 1024  # KB\n",
        "        test_size = os.path.getsize(test_file) / 1024\n",
        "        neg_size = os.path.getsize(neg_file) / 1024\n",
        "        \n",
        "        # Count lines\n",
        "        with open(train_file) as f:\n",
        "            train_lines = sum(1 for _ in f)\n",
        "        with open(test_file) as f:\n",
        "            test_lines = sum(1 for _ in f)\n",
        "        \n",
        "        # Get stats if available\n",
        "        if sample_size in results:\n",
        "            stats = results[sample_size]\n",
        "            num_users = stats['num_users']\n",
        "            num_items = stats['num_items']\n",
        "        else:\n",
        "            num_users = '?'\n",
        "            num_items = '?'\n",
        "        \n",
        "        file_sizes = f\"{train_size:.1f}K / {test_size:.1f}K / {neg_size:.1f}K\"\n",
        "        print(f\"{sample_name:<20} {str(num_users):<10} {str(num_items):<10} {train_lines:<15} {test_lines:<10} {file_sizes:<30}\")\n",
        "    else:\n",
        "        print(f\"{sample_name:<20} {'NOT FOUND':<60}\")\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"\\n\u2713 All sample datasets are ready for use.\")\n",
        "print(\"\\nUsage in notebooks:\")\n",
        "print(\"  DATASET = 'ml-1m-sample100'  # or 'ml-1m-sample500', 'ml-1m-sample1000'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}