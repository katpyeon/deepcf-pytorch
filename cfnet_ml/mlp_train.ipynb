{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (Multi-Layer Perceptron) 학습\n",
    "\n",
    "DeepCF 논문(AAAI 2019)의 CFNet-ml 모델을 PyTorch로 구현한 학습 노트북입니다.\n",
    "\n",
    "## 모델 개요\n",
    "\n",
    "MLP는 user-item embedding을 concatenate한 후 MLP로 학습하는 metric learning 기반 모델입니다.\n",
    "\n",
    "$$\\hat{y}_{ui} = \\sigma(W^T(f([r_u, r_i])) + b)$$\n",
    "\n",
    "- $r_u \\in \\{0,1\\}^{|I|}$: user $u$의 상호작용 벡터\n",
    "- $r_i \\in \\{0,1\\}^{|U|}$: item $i$의 상호작용 벡터\n",
    "- $f$: MLP (Multi-Layer Perceptron)\n",
    "- $[\\cdot, \\cdot]$: Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 임포트 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# 공통 유틸리티 임포트\n",
    "from common.data_utils import load_deepcf_data, get_train_matrix\n",
    "from common.train_utils import get_train_instances, TrainDataset\n",
    "from common.eval_utils import evaluate_model\n",
    "\n",
    "# CFNet-ml (MLP) 모델 임포트\n",
    "from cfnet_ml.mlp_model import MLP\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 디바이스 설정 (CUDA > MPS > CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_name = f\"CUDA ({torch.cuda.get_device_name(0)})\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    device_name = \"MPS (Apple Silicon)\"\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(f\"Device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ml-1m (Sample: True)\n",
      "Model: MLP[512, 256, 128, 64] (Embedding size: 256)\n",
      "Training: 20 epochs, batch=256, neg=4, lr=0.001\n"
     ]
    }
   ],
   "source": [
    "# 데이터 설정\n",
    "DATA_PATH = '../datasets/'\n",
    "DATASET = 'ml-1m'\n",
    "USE_SAMPLE = True\n",
    "SAMPLE_USERS = 100\n",
    "\n",
    "# 모델 구조 (논문 기본값)\n",
    "LAYERS = [512, 256, 128, 64]  # MLP 레이어 크기\n",
    "                              # layers[0]//2 = 256이 embedding 크기\n",
    "\n",
    "# 학습 파라미터\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "NUM_NEG = 4  # Negative sample 개수\n",
    "LEARNING_RATE = 0.001  # MLP는 0.001 사용 (DMF는 0.0001)\n",
    "LEARNER = 'adam'\n",
    "\n",
    "# 평가 및 저장\n",
    "TOP_K = 10\n",
    "VERBOSE = 1  # 평가 주기 (epoch)\n",
    "SAVE_MODEL = True\n",
    "MODEL_DIR = '../pretrain/'\n",
    "\n",
    "print(f\"Dataset: {DATASET} (Sample: {USE_SAMPLE})\")\n",
    "print(f\"Model: MLP{LAYERS} (Embedding size: {LAYERS[0]//2})\")\n",
    "print(f\"Training: {EPOCHS} epochs, batch={BATCH_SIZE}, neg={NUM_NEG}, lr={LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비\n",
    "\n",
    "샘플 데이터가 필요한 경우 `../data_sampling.ipynb`를 먼저 실행하세요.\n",
    "\n",
    "**데이터 포맷:**\n",
    "- `train.rating`, `test.rating`: `userID\\titemID\\trating\\ttimestamp`\n",
    "- `test.negative`: `(userID,itemID)\\tneg1\\tneg2\\t...` (99개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ml-1m-sample100...\n",
      "Loaded in 0.1s\n",
      "  Users: 100, Items: 2591\n",
      "  Train: 17361, Test: 93\n"
     ]
    }
   ],
   "source": [
    "dataset_name = f'{DATASET}-sample{SAMPLE_USERS}' if USE_SAMPLE else DATASET\n",
    "\n",
    "print(f\"Loading dataset: {dataset_name}...\")\n",
    "t1 = time()\n",
    "train, testRatings, testNegatives, num_users, num_items = load_deepcf_data(\n",
    "    DATA_PATH, dataset_name\n",
    ")\n",
    "print(f\"Loaded in {time()-t1:.1f}s\")\n",
    "print(f\"  Users: {num_users}, Items: {num_items}\")\n",
    "print(f\"  Train: {train.nnz}, Test: {len(testRatings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 생성\n",
    "\n",
    "**MLP 구조:**\n",
    "- User Embedding: $|I| \\rightarrow 256$ (linear)\n",
    "- Item Embedding: $|U| \\rightarrow 256$ (linear)\n",
    "- Concatenation: $[256, 256] \\rightarrow 512$\n",
    "- MLP: $512 \\rightarrow 256 \\rightarrow 128 \\rightarrow 64$ (ReLU)\n",
    "- Prediction: $64 \\rightarrow 1$ (Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 861,953 parameters\n",
      "Optimizer: adam, LR: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Train matrix 변환 (sparse -> dense)\n",
    "train_matrix = get_train_matrix(train)\n",
    "\n",
    "# 모델 초기화\n",
    "model = MLP(train_matrix, num_users, num_items, LAYERS).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Optimizer: {LEARNER}, LR: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 초기 성능 평가\n",
    "\n",
    "**평가 메트릭:**\n",
    "- Hit Ratio@K: Top-K 내 정답 포함 비율\n",
    "- NDCG@K: Normalized Discounted Cumulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating initial performance...\n",
      "Init: HR@10=0.2258, NDCG@10=0.1105 [0.3s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating initial performance...\")\n",
    "t1 = time()\n",
    "hits, ndcgs = evaluate_model(model, testRatings, testNegatives, TOP_K, device)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print(f\"Init: HR@{TOP_K}={hr:.4f}, NDCG@{TOP_K}={ndcg:.4f} [{time()-t1:.1f}s]\")\n",
    "\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습\n",
    "\n",
    "**학습 과정:**\n",
    "1. Negative sampling: 각 positive에 대해 $N$ 개의 negative 샘플 생성\n",
    "2. Mini-batch SGD로 BCE loss 최소화\n",
    "   $$\\mathcal{L} = -\\sum_{(u,i)} y_{ui} \\log \\hat{y}_{ui} + (1-y_{ui}) \\log(1-\\hat{y}_{ui})$$\n",
    "3. 매 epoch 평가 및 best model 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "================================================================================\n",
      "Epoch  0 [1.2s]: HR=0.3763, NDCG=0.2000, loss=0.3311 [0.1s]\n",
      "  → Best model saved (HR=0.3763)\n",
      "Epoch  1 [0.9s]: HR=0.4086, NDCG=0.1979, loss=0.1725 [0.1s]\n",
      "  → Best model saved (HR=0.4086)\n",
      "Epoch  2 [0.9s]: HR=0.3978, NDCG=0.2081, loss=0.1094 [0.1s]\n",
      "Epoch  3 [0.9s]: HR=0.3548, NDCG=0.1894, loss=0.1040 [0.1s]\n",
      "Epoch  4 [0.9s]: HR=0.3548, NDCG=0.1656, loss=0.0740 [0.1s]\n",
      "Epoch  5 [0.9s]: HR=0.3978, NDCG=0.1954, loss=0.0524 [0.1s]\n",
      "Epoch  6 [0.9s]: HR=0.4086, NDCG=0.2023, loss=0.0444 [0.1s]\n",
      "Epoch  7 [0.9s]: HR=0.4086, NDCG=0.1873, loss=0.0474 [0.1s]\n",
      "Epoch  8 [1.0s]: HR=0.4086, NDCG=0.1843, loss=0.0337 [0.1s]\n",
      "Epoch  9 [0.9s]: HR=0.3333, NDCG=0.1559, loss=0.0323 [0.1s]\n",
      "Epoch 10 [0.9s]: HR=0.3978, NDCG=0.1812, loss=0.0314 [0.1s]\n",
      "Epoch 11 [0.9s]: HR=0.3871, NDCG=0.1890, loss=0.0281 [0.1s]\n",
      "Epoch 12 [0.9s]: HR=0.3763, NDCG=0.1800, loss=0.0249 [0.1s]\n",
      "Epoch 13 [0.9s]: HR=0.3763, NDCG=0.1717, loss=0.0242 [0.1s]\n",
      "Epoch 14 [0.9s]: HR=0.3656, NDCG=0.2023, loss=0.0255 [0.1s]\n",
      "Epoch 15 [0.9s]: HR=0.4194, NDCG=0.2231, loss=0.0242 [0.1s]\n",
      "  → Best model saved (HR=0.4194)\n",
      "Epoch 16 [0.9s]: HR=0.3978, NDCG=0.1951, loss=0.0198 [0.1s]\n",
      "Epoch 17 [0.9s]: HR=0.3978, NDCG=0.1922, loss=0.0183 [0.1s]\n",
      "Epoch 18 [0.9s]: HR=0.3763, NDCG=0.1786, loss=0.0220 [0.1s]\n",
      "Epoch 19 [0.9s]: HR=0.3441, NDCG=0.1657, loss=0.0167 [0.1s]\n",
      "================================================================================\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장 디렉토리 생성\n",
    "if SAVE_MODEL:\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    # 데이터셋 이름을 prefix로 사용 (예: ml-1m-sample100-ml.pth)\n",
    "    model_out_file = f'{MODEL_DIR}{dataset_name}-ml.pth'\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t1 = time()\n",
    "    \n",
    "    # Negative sampling\n",
    "    user_input, item_input, labels = get_train_instances(train, NUM_NEG, num_items)\n",
    "    train_dataset = TrainDataset(user_input, item_input, labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_users, batch_items, batch_labels in train_loader:\n",
    "        batch_users = batch_users.to(device)\n",
    "        batch_items = batch_items.to(device)\n",
    "        batch_labels = batch_labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        predictions = model(batch_users, batch_items)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    t2 = time()\n",
    "    \n",
    "    # Evaluation\n",
    "    if epoch % VERBOSE == 0:\n",
    "        hits, ndcgs = evaluate_model(model, testRatings, testNegatives, TOP_K, device)\n",
    "        hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        \n",
    "        print(f'Epoch {epoch:2d} [{t2-t1:.1f}s]: HR={hr:.4f}, NDCG={ndcg:.4f}, '\n",
    "              f'loss={avg_loss:.4f} [{time()-t2:.1f}s]')\n",
    "        \n",
    "        # Best model 저장\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "            if SAVE_MODEL:\n",
    "                torch.save(model.state_dict(), model_out_file)\n",
    "                print(f'  → Best model saved (HR={hr:.4f})')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "Best Epoch: 15\n",
      "Best HR@10: 0.4194\n",
      "Best NDCG@10: 0.2231\n",
      "\n",
      "Model saved: ../pretrain/ml-1m-sample100-ml.pth\n",
      "\n",
      "Configuration:\n",
      "  Dataset: ml-1m-sample100\n",
      "  Architecture: MLP[512, 256, 128, 64]\n",
      "  Embedding size: 256\n",
      "  Training: 20 epochs, batch=256, neg=4\n",
      "  Optimizer: adam, lr=0.001\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Best Epoch: {best_iter}\")\n",
    "print(f\"Best HR@{TOP_K}: {best_hr:.4f}\")\n",
    "print(f\"Best NDCG@{TOP_K}: {best_ndcg:.4f}\")\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    print(f\"\\nModel saved: {model_out_file}\")\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Dataset: {dataset_name}\")\n",
    "print(f\"  Architecture: MLP{LAYERS}\")\n",
    "print(f\"  Embedding size: {LAYERS[0]//2}\")\n",
    "print(f\"  Training: {EPOCHS} epochs, batch={BATCH_SIZE}, neg={NUM_NEG}\")\n",
    "print(f\"  Optimizer: {LEARNER}, lr={LEARNING_RATE}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
