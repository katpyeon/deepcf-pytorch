{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFNet (Collaborative Filtering Network) 학습\n",
    "\n",
    "DeepCF 논문(AAAI 2019)의 최종 CFNet 모델을 PyTorch로 구현한 학습 노트북입니다.\n",
    "\n",
    "## 모델 개요\n",
    "\n",
    "CFNet은 DMF와 MLP를 결합한 앙상블 모델로, 두 접근법의 장점을 모두 활용합니다.\n",
    "\n",
    "$$\\hat{y}_{ui} = \\sigma(W^T([f_{DMF}(r_u, r_i), f_{MLP}(r_u, r_i)]) + b)$$\n",
    "\n",
    "- **DMF part**: Element-wise product (representation learning)\n",
    "- **MLP part**: Concatenation (metric learning)\n",
    "- **Fusion**: Concatenate DMF and MLP outputs\n",
    "\n",
    "## Pretrain (권장)\n",
    "\n",
    "논문에서는 DMF와 MLP를 먼저 개별 학습 후 가중치를 CFNet에 로드하는 방식을 권장합니다.\n",
    "이는 더 나은 초기값을 제공하여 최종 성능을 향상시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 임포트 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# 공통 유틸리티 임포트\n",
    "from common.data_utils import load_deepcf_data, get_train_matrix\n",
    "from common.train_utils import get_train_instances, TrainDataset\n",
    "from common.eval_utils import evaluate_model\n",
    "\n",
    "# CFNet 모델 명시적 임포트\n",
    "from cfnet.cfnet_model import CFNet\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 디바이스 설정 (CUDA > MPS > CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_name = f\"CUDA ({torch.cuda.get_device_name(0)})\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    device_name = \"MPS (Apple Silicon)\"\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(f\"Device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ml-1m (Sample: True)\n",
      "DMF: User[512, 64] / Item[1024, 64]\n",
      "MLP: [512, 256, 128, 64]\n",
      "Pretrain: True\n",
      "  - DMF: ../pretrain/ml-1m-sample100-rl.pth\n",
      "  - MLP: ../pretrain/ml-1m-sample100-ml.pth\n",
      "Training: 20 epochs, batch=256, neg=4, lr=0.0001\n"
     ]
    }
   ],
   "source": [
    "# 데이터 설정\n",
    "DATA_PATH = '../datasets/'\n",
    "DATASET = 'ml-1m'\n",
    "USE_SAMPLE = True\n",
    "SAMPLE_USERS = 100\n",
    "\n",
    "# DMF 구조 (CFNet의 DMF part)\n",
    "USERLAYERS = [512, 64]  # User tower 레이어 크기\n",
    "ITEMLAYERS = [1024, 64]  # Item tower 레이어 크기\n",
    "\n",
    "# MLP 구조 (CFNet의 MLP part)\n",
    "LAYERS = [512, 256, 128, 64]  # MLP 레이어 크기\n",
    "\n",
    "# Pretrain 설정 (중요!)\n",
    "USE_PRETRAIN = True  # True: pretrain 사용 (권장), False: from scratch\n",
    "DMF_PRETRAIN = '../pretrain/ml-1m-sample100-rl.pth'\n",
    "MLP_PRETRAIN = '../pretrain/ml-1m-sample100-ml.pth'\n",
    "\n",
    "# 학습 파라미터\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "NUM_NEG = 4  # Negative sample 개수\n",
    "LEARNING_RATE = 0.0001  # CFNet은 DMF와 같은 낮은 학습률 사용\n",
    "LEARNER = 'adam'\n",
    "\n",
    "# 평가 및 저장\n",
    "TOP_K = 10\n",
    "VERBOSE = 1  # 평가 주기 (epoch)\n",
    "SAVE_MODEL = True\n",
    "MODEL_DIR = '../pretrain/'\n",
    "\n",
    "print(f\"Dataset: {DATASET} (Sample: {USE_SAMPLE})\")\n",
    "print(f\"DMF: User{USERLAYERS} / Item{ITEMLAYERS}\")\n",
    "print(f\"MLP: {LAYERS}\")\n",
    "print(f\"Pretrain: {USE_PRETRAIN}\")\n",
    "if USE_PRETRAIN:\n",
    "    print(f\"  - DMF: {DMF_PRETRAIN}\")\n",
    "    print(f\"  - MLP: {MLP_PRETRAIN}\")\n",
    "print(f\"Training: {EPOCHS} epochs, batch={BATCH_SIZE}, neg={NUM_NEG}, lr={LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비\n",
    "\n",
    "샘플 데이터가 필요한 경우 `../data_sampling.ipynb`를 먼저 실행하세요.\n",
    "\n",
    "**데이터 포맷:**\n",
    "- `train.rating`, `test.rating`: `userID\\titemID\\trating\\ttimestamp`\n",
    "- `test.negative`: `(userID,itemID)\\tneg1\\tneg2\\t...` (99개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ml-1m-sample100...\n",
      "Loaded in 0.1s\n",
      "  Users: 100, Items: 2591\n",
      "  Train: 17361, Test: 93\n"
     ]
    }
   ],
   "source": [
    "dataset_name = f'{DATASET}-sample{SAMPLE_USERS}' if USE_SAMPLE else DATASET\n",
    "\n",
    "print(f\"Loading dataset: {dataset_name}...\")\n",
    "t1 = time()\n",
    "train, testRatings, testNegatives, num_users, num_items = load_deepcf_data(\n",
    "    DATA_PATH, dataset_name\n",
    ")\n",
    "print(f\"Loaded in {time()-t1:.1f}s\")\n",
    "print(f\"  Users: {num_users}, Items: {num_items}\")\n",
    "print(f\"  Train: {train.nnz}, Test: {len(testRatings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 생성\n",
    "\n",
    "**CFNet 구조:**\n",
    "- DMF part:\n",
    "  - User tower: $|I| \\rightarrow 512 \\rightarrow 64$\n",
    "  - Item tower: $|U| \\rightarrow 1024 \\rightarrow 64$\n",
    "  - DMF vector: element-wise product (64 dim)\n",
    "- MLP part:\n",
    "  - User/Item embedding: $|I|, |U| \\rightarrow 256$\n",
    "  - Concat: $[256, 256] \\rightarrow 512$\n",
    "  - MLP: $512 \\rightarrow 256 \\rightarrow 128 \\rightarrow 64$\n",
    "- Fusion:\n",
    "  - Concat: $[64_{DMF}, 64_{MLP}] \\rightarrow 128$\n",
    "  - Prediction: $\\text{sigmoid}(Linear(128, 1))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained models...\n",
      "✓ Pretrain loaded from:\n",
      "  - DMF: ../pretrain/ml-1m-sample100-rl.pth\n",
      "  - MLP: ../pretrain/ml-1m-sample100-ml.pth\n",
      "\n",
      "Model: 2,390,977 parameters\n",
      "Optimizer: adam, LR: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Train matrix 변환 (sparse -> dense)\n",
    "train_matrix = get_train_matrix(train)\n",
    "\n",
    "# 모델 초기화 (Pretrain 또는 From Scratch)\n",
    "if USE_PRETRAIN:\n",
    "    print(f\"Loading pretrained models...\")\n",
    "    model = CFNet(train_matrix, num_users, num_items,\n",
    "                  USERLAYERS, ITEMLAYERS, LAYERS,\n",
    "                  dmf_pretrain_path=DMF_PRETRAIN,\n",
    "                  mlp_pretrain_path=MLP_PRETRAIN).to(device)\n",
    "    print(f\"✓ Pretrain loaded from:\")\n",
    "    print(f\"  - DMF: {DMF_PRETRAIN}\")\n",
    "    print(f\"  - MLP: {MLP_PRETRAIN}\")\n",
    "else:\n",
    "    print(f\"Initializing from scratch...\")\n",
    "    model = CFNet(train_matrix, num_users, num_items,\n",
    "                  USERLAYERS, ITEMLAYERS, LAYERS).to(device)\n",
    "    print(f\"✓ Random initialization (Lecun normal)\")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"\\nModel: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Optimizer: {LEARNER}, LR: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 초기 성능 평가\n",
    "\n",
    "**평가 메트릭:**\n",
    "- Hit Ratio@K: Top-K 내 정답 포함 비율\n",
    "- NDCG@K: Normalized Discounted Cumulative Gain\n",
    "\n",
    "**Pretrain vs From Scratch:**\n",
    "- Pretrain 사용 시: 초기 성능이 DMF, MLP보다 높음 (가중치 물려받음)\n",
    "- From scratch: 초기 성능이 매우 낮음 (랜덤 초기화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating initial performance...\n",
      "Init: HR@10=0.4086, NDCG@10=0.2346 [0.5s]\n",
      "\n",
      "✓ Pretrain 효과: 초기 성능이 DMF/MLP보다 높아야 함\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating initial performance...\")\n",
    "t1 = time()\n",
    "hits, ndcgs = evaluate_model(model, testRatings, testNegatives, TOP_K, device)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print(f\"Init: HR@{TOP_K}={hr:.4f}, NDCG@{TOP_K}={ndcg:.4f} [{time()-t1:.1f}s]\")\n",
    "\n",
    "if USE_PRETRAIN:\n",
    "    print(f\"\\n✓ Pretrain 효과: 초기 성능이 DMF/MLP보다 높아야 함\")\n",
    "else:\n",
    "    print(f\"\\n✓ From scratch: 초기 성능이 낮음 (랜덤 초기화)\")\n",
    "\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습\n",
    "\n",
    "**학습 과정:**\n",
    "1. Negative sampling: 각 positive에 대해 $N$ 개의 negative 샘플 생성\n",
    "2. Mini-batch SGD로 BCE loss 최소화\n",
    "   $$\\mathcal{L} = -\\sum_{(u,i)} y_{ui} \\log \\hat{y}_{ui} + (1-y_{ui}) \\log(1-\\hat{y}_{ui})$$\n",
    "3. 매 epoch 평가 및 best model 저장\n",
    "\n",
    "**예상 성능 (논문 기준):**\n",
    "- DMF: HR@10 ≈ 0.68\n",
    "- MLP: HR@10 ≈ 0.69\n",
    "- **CFNet (pretrain)**: HR@10 ≈ 0.70 ⭐ (최고 성능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "================================================================================\n",
      "Epoch  0 [3.0s]: HR=0.4516, NDCG=0.2183, loss=0.0122 [0.1s]\n",
      "  → Best performance updated (HR=0.4516)\n",
      "Epoch  1 [1.3s]: HR=0.4409, NDCG=0.2030, loss=0.0074 [0.1s]\n",
      "Epoch  2 [1.3s]: HR=0.4516, NDCG=0.2125, loss=0.0057 [0.1s]\n",
      "Epoch  3 [1.3s]: HR=0.4409, NDCG=0.2050, loss=0.0054 [0.1s]\n",
      "Epoch  4 [1.3s]: HR=0.4301, NDCG=0.2021, loss=0.0043 [0.1s]\n",
      "Epoch  5 [1.2s]: HR=0.4194, NDCG=0.1993, loss=0.0040 [0.1s]\n",
      "Epoch  6 [1.3s]: HR=0.4301, NDCG=0.2115, loss=0.0039 [0.1s]\n",
      "Epoch  7 [1.2s]: HR=0.4194, NDCG=0.2026, loss=0.0036 [0.1s]\n",
      "Epoch  8 [1.2s]: HR=0.4409, NDCG=0.2083, loss=0.0026 [0.1s]\n",
      "Epoch  9 [1.2s]: HR=0.3763, NDCG=0.1893, loss=0.0025 [0.1s]\n",
      "Epoch 10 [1.2s]: HR=0.4301, NDCG=0.2052, loss=0.0025 [0.1s]\n",
      "Epoch 11 [1.2s]: HR=0.3978, NDCG=0.1936, loss=0.0021 [0.1s]\n",
      "Epoch 12 [1.2s]: HR=0.3978, NDCG=0.1947, loss=0.0020 [0.1s]\n",
      "Epoch 13 [1.2s]: HR=0.4086, NDCG=0.1891, loss=0.0017 [0.1s]\n",
      "Epoch 14 [1.2s]: HR=0.4301, NDCG=0.1996, loss=0.0015 [0.1s]\n",
      "Epoch 15 [1.2s]: HR=0.3978, NDCG=0.2075, loss=0.0019 [0.1s]\n",
      "Epoch 16 [1.2s]: HR=0.4194, NDCG=0.2067, loss=0.0022 [0.1s]\n",
      "Epoch 17 [1.2s]: HR=0.4086, NDCG=0.1860, loss=0.0024 [0.1s]\n",
      "Epoch 18 [1.3s]: HR=0.3656, NDCG=0.1702, loss=0.0019 [0.1s]\n",
      "Epoch 19 [1.2s]: HR=0.3871, NDCG=0.1856, loss=0.0016 [0.1s]\n",
      "================================================================================\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# CFNet은 DMF + MLP fusion 모델이므로 별도로 저장하지 않음\n",
    "# (DMF-rl.pth와 MLP-ml.pth만 있으면 CFNet 재구성 가능)\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t1 = time()\n",
    "    \n",
    "    # Negative sampling\n",
    "    user_input, item_input, labels = get_train_instances(train, NUM_NEG, num_items)\n",
    "    train_dataset = TrainDataset(user_input, item_input, labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_users, batch_items, batch_labels in train_loader:\n",
    "        batch_users = batch_users.to(device)\n",
    "        batch_items = batch_items.to(device)\n",
    "        batch_labels = batch_labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        predictions = model(batch_users, batch_items)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    t2 = time()\n",
    "    \n",
    "    # Evaluation\n",
    "    if epoch % VERBOSE == 0:\n",
    "        hits, ndcgs = evaluate_model(model, testRatings, testNegatives, TOP_K, device)\n",
    "        hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        \n",
    "        print(f'Epoch {epoch:2d} [{t2-t1:.1f}s]: HR={hr:.4f}, NDCG={ndcg:.4f}, '\n",
    "              f'loss={avg_loss:.4f} [{time()-t2:.1f}s]')\n",
    "        \n",
    "        # Best performance tracking (저장하지 않음)\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "            print(f'  → Best performance updated (HR={hr:.4f})')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "Best Epoch: 0\n",
      "Best HR@10: 0.4516\n",
      "Best NDCG@10: 0.2183\n",
      "\n",
      "Configuration:\n",
      "  Dataset: ml-1m-sample100\n",
      "  DMF: User[512, 64], Item[1024, 64]\n",
      "  MLP: [512, 256, 128, 64]\n",
      "  Pretrain: True\n",
      "  Training: 20 epochs, batch=256, neg=4\n",
      "  Optimizer: adam, lr=0.0001\n",
      "\n",
      "================================================================================\n",
      "성능 비교 (참고):\n",
      "  DMF (sample-100): HR@10 ≈ 0.79\n",
      "  MLP (sample-100): HR@10 ≈ 0.72\n",
      "  CFNet (현재):    HR@10 = 0.4516\n",
      "\n",
      "✓ CFNet이 DMF, MLP보다 높은 성능을 보이면 성공!\n",
      "\n",
      "Note: CFNet은 별도로 저장하지 않습니다.\n",
      "      (DMF와 MLP pretrain 가중치만 있으면 CFNet 재구성 가능)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Best Epoch: {best_iter}\")\n",
    "print(f\"Best HR@{TOP_K}: {best_hr:.4f}\")\n",
    "print(f\"Best NDCG@{TOP_K}: {best_ndcg:.4f}\")\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Dataset: {dataset_name}\")\n",
    "print(f\"  DMF: User{USERLAYERS}, Item{ITEMLAYERS}\")\n",
    "print(f\"  MLP: {LAYERS}\")\n",
    "print(f\"  Pretrain: {USE_PRETRAIN}\")\n",
    "print(f\"  Training: {EPOCHS} epochs, batch={BATCH_SIZE}, neg={NUM_NEG}\")\n",
    "print(f\"  Optimizer: {LEARNER}, lr={LEARNING_RATE}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"성능 비교 (참고):\")\n",
    "print(\"  DMF (sample-100): HR@10 ≈ 0.79\")\n",
    "print(\"  MLP (sample-100): HR@10 ≈ 0.72\")\n",
    "print(f\"  CFNet (현재):    HR@10 = {best_hr:.4f}\")\n",
    "print(\"\\n✓ CFNet이 DMF, MLP보다 높은 성능을 보이면 성공!\")\n",
    "print(\"\\nNote: CFNet은 별도로 저장하지 않습니다.\")\n",
    "print(\"      (DMF와 MLP pretrain 가중치만 있으면 CFNet 재구성 가능)\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
