# CFNet Pretrain Prediction Layer ì´ˆê¸°í™” ë¡œì§ ì°¨ì´

## ğŸ“Œ ìš”ì•½

**ì›ë³¸ TensorFlow**ì™€ **í˜„ì¬ PyTorch êµ¬í˜„**ì˜ CFNet pretrain ë¡œë“œ ì‹œ prediction layer ì´ˆê¸°í™” ë°©ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤.

- **ì›ë³¸**: DMFì™€ MLP prediction ê°€ì¤‘ì¹˜ë¥¼ ë³µì¡í•œ 2ë‹¨ê³„ ê³¼ì •ìœ¼ë¡œ ê²°í•©
- **í˜„ì¬**: DMFì™€ MLP prediction ê°€ì¤‘ì¹˜ë¥¼ ë‹¨ìˆœ concatenate

**ê²°ë¡ **: **í˜„ì¬ PyTorch êµ¬í˜„ (ì˜µì…˜ 2) ìœ ì§€ ê¶Œì¥** âœ…
- Forward passëŠ” 100% ë™ì¼ (ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •í™•)
- Pretrain ì´ˆê¸°í™”ëŠ” í•™ìŠµ ì‹œì‘ì ì˜ ì°¨ì´ì¼ ë¿
- í˜„ì¬ êµ¬í˜„ì´ ë” ì§ê´€ì ì´ê³  ë…¼ë¦¬ì 
- ì›ë³¸ ë¡œì§ì˜ ë¹„ëŒ€ì¹­ì„±ì´ ì˜¤íˆë ¤ ì˜ë¬¸

---

## ğŸ” ìƒì„¸ ë¹„êµ

### 1. ì›ë³¸ TensorFlow ë¡œì§ (CFNet.py)

#### Step 1: DMF pretrain ë¡œë“œ í›„ (105-124í–‰)

```python
def load_pretrain_model1(model, dmf_model, dmf_layers):
    # DMF user/item layers ë³µì‚¬ (107-117í–‰)
    # ... (ìƒëµ)

    # Prediction weights ì´ˆê¸°í™” (119-123í–‰)
    dmf_prediction = dmf_model.get_layer('prediction').get_weights()
    # dmf_prediction[0].shape = [dmf_layers[-1], 1]  ì˜ˆ: [64, 1]
    # dmf_prediction[1].shape = [1]  (bias)

    new_weights = np.concatenate(
        (dmf_prediction[0], np.array([[0,]] * dmf_layers[-1])),
        axis=0
    )
    # ê²°ê³¼: [dmf_layers[-1] + dmf_layers[-1], 1] = [128, 1]
    #       ì• 64ê°œ = DMF prediction weights
    #       ë’¤ 64ê°œ = 0ìœ¼ë¡œ ì´ˆê¸°í™” (MLP ë¶€ë¶„)

    new_b = dmf_prediction[1]
    model.get_layer('prediction').set_weights([new_weights, new_b])

    return model
```

**ì˜ë¬¸ì **: ì™œ `dmf_layers[-1]`ê°œ ë§Œí¼ 0ì„ ì¶”ê°€? â†’ MLP output dimì€ `layers[-1]`ì¸ë°?

---

#### Step 2: MLP pretrain ë¡œë“œ í›„ (126-145í–‰)

```python
def load_pretrain_model2(model, mlp_model, mlp_layers):
    # MLP embedding/layers ë³µì‚¬ (128-136í–‰)
    # ... (ìƒëµ)

    # Prediction weights ì¬ì´ˆê¸°í™” (138-144í–‰)
    dmf_prediction = model.get_layer('prediction').get_weights()
    # ìœ„ Step 1ì—ì„œ ì„¤ì •ëœ [128, 1] ê°€ì¤‘ì¹˜

    mlp_prediction = mlp_model.get_layer('prediction').get_weights()
    # mlp_prediction[0].shape = [mlp_layers[-1], 1]  ì˜ˆ: [64, 1]

    new_weights = np.concatenate(
        (dmf_prediction[0][:mlp_layers[-1]], mlp_prediction[0]),
        axis=0
    )
    # ê²°ê³¼: [mlp_layers[-1] + mlp_layers[-1], 1] = [64 + 64, 1] = [128, 1]
    #       ì• 64ê°œ = dmf_prediction[0]ì˜ ì• 64ê°œ (DMF ê°€ì¤‘ì¹˜ ì¼ë¶€)
    #       ë’¤ 64ê°œ = MLP prediction weights

    new_b = dmf_prediction[1] + mlp_prediction[1]
    # biasëŠ” ë‹¨ìˆœ í•©

    # 0.5 means the contributions of MF and MLP are equal (143í–‰ ì£¼ì„)
    model.get_layer('prediction').set_weights([0.5*new_weights, 0.5*new_b])

    return model
```

**í•µì‹¬ ë¡œì§**:
```python
dmf_prediction[0][:mlp_layers[-1]]  # DMF ê°€ì¤‘ì¹˜ì˜ ì• mlp_layers[-1]ê°œë§Œ ì„ íƒ
```

---

### 2. í˜„ì¬ PyTorch êµ¬í˜„ (cfnet_model.py 167-252í–‰)

```python
def _load_pretrained_weights(self, dmf_path, mlp_path):
    # íŒŒì¼ ë¡œë“œ
    dmf_state = torch.load(dmf_path, map_location='cpu')
    mlp_state = torch.load(mlp_path, map_location='cpu')

    # DMF layers ë³µì‚¬ (194-210í–‰)
    for idx in range(self.dmf_num_layer):
        # user_layers, item_layers ë³µì‚¬
        # ...

    # DMF prediction layer ê°€ì¤‘ì¹˜ ì¶”ì¶œ
    dmf_pred_weight = dmf_state['prediction.weight']  # [1, dmf_dim]
    dmf_pred_bias = dmf_state['prediction.bias']      # [1]

    # MLP layers ë³µì‚¬ (218-229í–‰)
    # user_embedding, item_embedding, mlp_layers ë³µì‚¬
    # ...

    # MLP prediction layer ê°€ì¤‘ì¹˜ ì¶”ì¶œ
    mlp_pred_weight = mlp_state['prediction.weight']  # [1, mlp_dim]
    mlp_pred_bias = mlp_state['prediction.bias']      # [1]

    # Prediction layer ì´ˆê¸°í™” (246-252í–‰)
    new_weight = torch.cat([dmf_pred_weight, mlp_pred_weight], dim=1)
    # [1, dmf_dim + mlp_dim] = [1, 128]

    new_bias = 0.5 * (dmf_pred_bias + mlp_pred_bias)

    self.prediction.weight.data.copy_(0.5 * new_weight)
    self.prediction.bias.data.copy_(new_bias)
```

**í•µì‹¬ ë¡œì§**:
```python
torch.cat([dmf_pred_weight, mlp_pred_weight], dim=1)  # ë‹¨ìˆœ concatenate
```

---

## ğŸ“Š ì°¨ì´ì  ë¶„ì„

### Shape ë¹„êµ

**ê°€ì •**: `userlayers=[512, 64]`, `itemlayers=[1024, 64]`, `layers=[512, 256, 128, 64]`

| ë‹¨ê³„ | ì›ë³¸ TensorFlow | í˜„ì¬ PyTorch |
|-----|----------------|--------------|
| **DMF output dim** | 64 (userlayers[-1]) | 64 |
| **MLP output dim** | 64 (layers[-1]) | 64 |
| **CFNet prediction input** | 128 (DMF 64 + MLP 64) | 128 (DMF 64 + MLP 64) |
| | | |
| **DMF prediction weights** | [64, 1] | [1, 64] (transpose) |
| **MLP prediction weights** | [64, 1] | [1, 64] (transpose) |
| | | |
| **Step 1 (DMF ë¡œë“œ í›„)** | [128, 1] = [64(DMF) + 64(zeros)] | - |
| **Step 2 (MLP ë¡œë“œ í›„)** | [128, 1] = [64(DMF[:64]) + 64(MLP)] | [1, 128] = [64(DMF) + 64(MLP)] |
| | | |
| **ìµœì¢… scaling** | 0.5 * weights | 0.5 * weights |
| **ìµœì¢… bias** | 0.5 * (DMF bias + MLP bias) | 0.5 * (DMF bias + MLP bias) |

---

### ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ê°’ ë¹„êµ

**ì›ë³¸ TensorFlow**:
```
Prediction weight = 0.5 * [
    dmf_weights[0:64],   â† DMF predictionì˜ ì• 64ê°œ
    mlp_weights[0:64]    â† MLP prediction ì „ì²´
]
```

**í˜„ì¬ PyTorch**:
```
Prediction weight = 0.5 * [
    dmf_weights[0:64],   â† DMF prediction ì „ì²´
    mlp_weights[0:64]    â† MLP prediction ì „ì²´
]
```

**ì°¨ì´ì **: ì—†ìŒ! (dmf_layers[-1] == mlp_layers[-1] == 64ì¸ ê²½ìš°)

---

### âš ï¸ ì›ë³¸ ë¡œì§ì˜ ë¬¸ì œì 

#### 1. ë¹„ëŒ€ì¹­ì  ì´ˆê¸°í™”

```python
# Step 1: dmf_layers[-1]ê°œ ë§Œí¼ 0 ì¶”ê°€
new_weights = np.concatenate(
    (dmf_prediction[0], np.array([[0,]] * dmf_layers[-1])),  # â† dmf_layers[-1]
    axis=0
)

# Step 2: mlp_layers[-1]ê°œë§Œ ì„ íƒ
new_weights = np.concatenate(
    (dmf_prediction[0][:mlp_layers[-1]], mlp_prediction[0]),  # â† mlp_layers[-1]
    axis=0
)
```

**ì˜ë¬¸**:
- Step 1ì—ì„œëŠ” `dmf_layers[-1]`ë¥¼ ì‚¬ìš©
- Step 2ì—ì„œëŠ” `mlp_layers[-1]`ë¥¼ ì‚¬ìš©
- ë‘ ê°’ì´ ë‹¤ë¥´ë©´? (ì˜ˆ: dmf=64, mlp=128)

#### 2. ì‹¤ì œ CFNet prediction input dimension

CFNet forward pass (CFNet.py 87í–‰):
```python
predict_vector = concatenate([dmf_vector, mlp_vector])
# dmf_vector: [batch, userlayers[-1]] = [batch, 64]
# mlp_vector: [batch, layers[-1]] = [batch, 64]
# predict_vector: [batch, 128]
```

**ë”°ë¼ì„œ**: Prediction layer input = `userlayers[-1] + layers[-1]` = 128

**ì›ë³¸ Step 1ì˜ ë¬¸ì œ**:
- `dmf_layers[-1]` (64) + `dmf_layers[-1]` (64) = 128 âœ… (ìš°ì—°íˆ ë§ìŒ)
- í•˜ì§€ë§Œ `mlp_layers[-1]`ë¥¼ ì‚¬ìš©í•´ì•¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë§ìŒ

---

## ğŸ’¡ í˜„ì¬ PyTorch êµ¬í˜„ì´ ë” ë‚˜ì€ ì´ìœ 

### 1. ì§ê´€ì 

```python
# DMFì˜ prediction ê°€ì¤‘ì¹˜ + MLPì˜ prediction ê°€ì¤‘ì¹˜ = CFNetì˜ prediction ê°€ì¤‘ì¹˜
new_weight = torch.cat([dmf_pred_weight, mlp_pred_weight], dim=1)
```

**ì˜ë¯¸**: CFNetì€ DMFì™€ MLPë¥¼ concatenateí•˜ë¯€ë¡œ, ê°ê°ì˜ prediction ê°€ì¤‘ì¹˜ë„ concatenateí•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ¬ì›€

---

### 2. ë…¼ë¦¬ì  ì¼ê´€ì„±

- Forward pass: `[dmf_vector, mlp_vector]` concatenate
- Pretrain ì´ˆê¸°í™”: `[dmf_weights, mlp_weights]` concatenate
- **ì™„ë²½í•œ ëŒ€ì‘**

---

### 3. ì¼ë°˜ì„±

ì›ë³¸ì€ `userlayers[-1] == layers[-1]`ì¼ ë•Œë§Œ ì œëŒ€ë¡œ ì‘ë™:
```python
# ë§Œì•½ userlayers[-1]=64, layers[-1]=128ì´ë©´?
# Step 1: [64, 1] + [64, 1](zeros) = [128, 1]  â† ì˜ëª»ëœ í¬ê¸°!
# Step 2: [128, 1][:128] + [128, 1] = [128 + 128, 1] = [256, 1]  â† ì—‰ë§!
```

í˜„ì¬ PyTorch êµ¬í˜„ì€ ëª¨ë“  ê²½ìš°ì— ì‘ë™:
```python
# userlayers[-1]=64, layers[-1]=128ì´ë©´
new_weight = [1, 64] + [1, 128] = [1, 192]  â† í•­ìƒ ì˜¬ë°”ë¥¸ í¬ê¸°
```

---

### 4. Forward passì™€ 100% ì¼ì¹˜

ê°€ì¥ ì¤‘ìš”í•œ ì : **Forward passëŠ” ì›ë³¸ê³¼ ì™„ì „íˆ ë™ì¼**

```python
# ì›ë³¸ CFNet.py 87-91í–‰
predict_vector = concatenate([dmf_vector, mlp_vector])
prediction = Dense(1, activation='sigmoid')(predict_vector)

# PyTorch cfnet_model.py 308-311í–‰
predict_vector = torch.cat([dmf_vector, mlp_vector], dim=-1)
prediction = torch.sigmoid(self.prediction(predict_vector))
```

**ê²°ë¡ **: Pretrain ì´ˆê¸°í™”ëŠ” í•™ìŠµ **ì‹œì‘ì **ì¼ ë¿, ëª¨ë¸ êµ¬ì¡°ëŠ” ë™ì¼í•˜ë¯€ë¡œ ìµœì¢… ì„±ëŠ¥ì— í° ì˜í–¥ ì—†ìŒ

---

## ğŸ”§ ì˜µì…˜ 1: ì›ë³¸ TensorFlow ë¡œì§ ì¬í˜„ (ì°¸ê³ ìš©)

í•„ìš” ì‹œ ë‹¤ìŒ ì½”ë“œë¡œ ì›ë³¸ ë¡œì§ì„ ì •í™•íˆ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ìˆ˜ì • ë°©ë²•

`cfnet/cfnet_model.py`ì˜ `_load_pretrained_weights()` ë©”ì„œë“œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •:

```python
def _load_pretrained_weights(self, dmf_path, mlp_path):
    """
    ì‚¬ì „ í•™ìŠµëœ DMFì™€ MLP ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    ì›ë³¸ TensorFlow ë¡œì§ì„ ì •í™•íˆ ì¬í˜„í•©ë‹ˆë‹¤.
    """
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(dmf_path):
        raise FileNotFoundError(f"DMF pretrain íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dmf_path}")
    if not os.path.exists(mlp_path):
        raise FileNotFoundError(f"MLP pretrain íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mlp_path}")

    dmf_state = torch.load(dmf_path, map_location='cpu')
    mlp_state = torch.load(mlp_path, map_location='cpu')

    # ============================================================
    # DMF ê°€ì¤‘ì¹˜ ë¡œë“œ (ì›ë³¸ 105-124 ë¼ì¸)
    # ============================================================

    # User tower ê°€ì¤‘ì¹˜ ë³µì‚¬
    for idx in range(self.dmf_num_layer):
        layer_weight = dmf_state[f'user_layers.{idx}.weight']
        layer_bias = dmf_state[f'user_layers.{idx}.bias']
        self.dmf_user_layers[idx].weight.data.copy_(layer_weight)
        self.dmf_user_layers[idx].bias.data.copy_(layer_bias)

    # Item tower ê°€ì¤‘ì¹˜ ë³µì‚¬
    for idx in range(self.dmf_num_layer):
        layer_weight = dmf_state[f'item_layers.{idx}.weight']
        layer_bias = dmf_state[f'item_layers.{idx}.bias']
        self.dmf_item_layers[idx].weight.data.copy_(layer_weight)
        self.dmf_item_layers[idx].bias.data.copy_(layer_bias)

    # DMF prediction layer ê°€ì¤‘ì¹˜
    dmf_pred_weight = dmf_state['prediction.weight']  # [1, dmf_dim]
    dmf_pred_bias = dmf_state['prediction.bias']      # [1]

    # ì›ë³¸ 121-122í–‰ ë¡œì§: DMF ê°€ì¤‘ì¹˜ + 0ìœ¼ë¡œ ì±„ìš°ê¸°
    dmf_dim = dmf_pred_weight.shape[1]  # userlayers[-1]
    zeros_part = torch.zeros(1, dmf_dim)  # [1, dmf_dim]

    # Step 1 ê°€ì¤‘ì¹˜: [1, dmf_dim + dmf_dim]
    step1_weight = torch.cat([dmf_pred_weight, zeros_part], dim=1)
    step1_bias = dmf_pred_bias

    # ============================================================
    # MLP ê°€ì¤‘ì¹˜ ë¡œë“œ (ì›ë³¸ 126-145 ë¼ì¸)
    # ============================================================

    # Embedding layer ê°€ì¤‘ì¹˜ ë³µì‚¬
    self.mlp_user_embedding.weight.data.copy_(mlp_state['user_embedding.weight'])
    self.mlp_user_embedding.bias.data.copy_(mlp_state['user_embedding.bias'])
    self.mlp_item_embedding.weight.data.copy_(mlp_state['item_embedding.weight'])
    self.mlp_item_embedding.bias.data.copy_(mlp_state['item_embedding.bias'])

    # MLP layer ê°€ì¤‘ì¹˜ ë³µì‚¬
    for idx in range(self.mlp_num_layer - 1):
        layer_weight = mlp_state[f'mlp_layers.{idx}.weight']
        layer_bias = mlp_state[f'mlp_layers.{idx}.bias']
        self.mlp_layers[idx].weight.data.copy_(layer_weight)
        self.mlp_layers[idx].bias.data.copy_(layer_bias)

    # MLP prediction layer ê°€ì¤‘ì¹˜
    mlp_pred_weight = mlp_state['prediction.weight']  # [1, mlp_dim]
    mlp_pred_bias = mlp_state['prediction.bias']      # [1]

    # ì›ë³¸ 141-144í–‰ ë¡œì§: DMF ì¼ë¶€ + MLP ì „ì²´
    mlp_dim = mlp_pred_weight.shape[1]  # layers[-1]

    # Step 1 ê°€ì¤‘ì¹˜ì˜ ì• mlp_dimê°œë§Œ ì‚¬ìš©
    dmf_part = step1_weight[:, :mlp_dim]  # [1, mlp_dim]

    # ìµœì¢… ê°€ì¤‘ì¹˜: [1, mlp_dim + mlp_dim]
    new_weight = torch.cat([dmf_part, mlp_pred_weight], dim=1)
    new_bias = step1_bias + mlp_pred_bias

    # 0.5 scaling (ì›ë³¸ 144í–‰)
    self.prediction.weight.data.copy_(0.5 * new_weight)
    self.prediction.bias.data.copy_(0.5 * new_bias)
```

---

### ì›ë³¸ ì¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­

#### 1. userlayers[-1] != layers[-1]ì¸ ê²½ìš°

ì›ë³¸ ë¡œì§ì€ ë‹¤ìŒ ì¡°ê±´ì„ **ê°€ì •**:
```python
userlayers[-1] == itemlayers[-1] == layers[-1]
```

ì´ ì¡°ê±´ì´ ê¹¨ì§€ë©´ ì›ë³¸ ë¡œì§ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŒ:

**ì˜ˆì‹œ**: `userlayers=[512, 64]`, `itemlayers=[1024, 64]`, `layers=[512, 256, 128, 128]`

```python
# DMF output: 64
# MLP output: 128
# CFNet prediction input: 64 + 128 = 192

# ì›ë³¸ Step 1: [64, 1] + [64(zeros), 1] = [128, 1]  â† ì˜ëª»ë¨! (192ì—¬ì•¼ í•¨)
# ì›ë³¸ Step 2: [128, 1][:128] + [128, 1] = [256, 1]  â† ì™„ì „íˆ ì˜ëª»ë¨!
```

#### 2. TensorFlowì™€ PyTorchì˜ shape ì°¨ì´

- TensorFlow: `[output_dim, 1]`
- PyTorch: `[1, output_dim]` (transpose)

ì›ë³¸ ì¬í˜„ ì‹œ ì£¼ì˜ í•„ìš”

---

## ğŸ¯ ê¶Œì¥ ì‚¬í•­

### âœ… í˜„ì¬ êµ¬í˜„ (ì˜µì…˜ 2) ìœ ì§€

**ì´ìœ **:
1. **Forward pass ë™ì¼**: ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” ì›ë³¸ê³¼ 100% ì¼ì¹˜
2. **ë” ì§ê´€ì **: DMF + MLPë¥¼ ë‹¨ìˆœ concatenate
3. **ë…¼ë¦¬ì  ì¼ê´€ì„±**: Forwardì™€ pretrain ì´ˆê¸°í™” ë°©ì‹ì´ ëŒ€ì‘
4. **ì¼ë°˜ì„±**: ëª¨ë“  layer í¬ê¸° ì¡°í•©ì— ëŒ€í•´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™
5. **Pretrainì€ ì´ˆê¸°í™”**: í•™ìŠµ í›„ì—ëŠ” ìˆ˜ë ´í•˜ë¯€ë¡œ ìµœì¢… ì„±ëŠ¥ì— í° ì˜í–¥ ì—†ìŒ

### âš ï¸ ì›ë³¸ ì¬í˜„ (ì˜µì…˜ 1)ì€ ë‹¤ìŒ ê²½ìš°ì—ë§Œ ê³ ë ¤

- ì›ë³¸ TensorFlow ì½”ë“œì™€ **ì™„ì „íˆ ë™ì¼í•œ ì‹¤í—˜ ì¬í˜„**ì´ í•„ìš”í•œ ê²½ìš°
- ë…¼ë¬¸ ì €ìì˜ ê³µì‹ êµ¬í˜„ì„ **ê·¸ëŒ€ë¡œ** ë”°ë¼ì•¼ í•˜ëŠ” ê²½ìš°
- í•˜ì§€ë§Œ ì›ë³¸ ë¡œì§ ìì²´ê°€ ë¹„ì¼ê´€ì ì´ë¯€ë¡œ ê¶Œì¥í•˜ì§€ ì•ŠìŒ

---

## ğŸ“š ì°¸ê³ : ì›ë³¸ TensorFlow ì½”ë“œ ìœ„ì¹˜

- **íŒŒì¼**: `/Users/yeonghyeonchoe/dev/python/DeepCF-master/CFNet.py`
- **DMF pretrain ë¡œë“œ**: 105-124í–‰ (`load_pretrain_model1`)
- **MLP pretrain ë¡œë“œ**: 126-145í–‰ (`load_pretrain_model2`)
- **Forward pass**: 49-96í–‰ (`get_model`)

---

## ğŸ”¬ ì‹¤í—˜ ê²€ì¦ (ì„ íƒ ì‚¬í•­)

ë‘ ë°©ì‹ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ í™•ì¸í•˜ë ¤ë©´:

1. í˜„ì¬ êµ¬í˜„ (ì˜µì…˜ 2)ë¡œ í•™ìŠµ â†’ ì„±ëŠ¥ A ê¸°ë¡
2. ì˜µì…˜ 1ë¡œ ìˆ˜ì • í›„ í•™ìŠµ â†’ ì„±ëŠ¥ B ê¸°ë¡
3. Aì™€ B ë¹„êµ:
   - ì°¨ì´ê°€ ë¯¸ë¯¸í•˜ë©´ (< 0.01): ì˜µì…˜ 2 ìœ ì§€
   - ì°¨ì´ê°€ í¬ë©´ (> 0.05): ì›ë³¸ ë¡œì§ ê²€í†  ë° ë…¼ë¬¸ í™•ì¸

**ì˜ˆìƒ**: ì°¨ì´ ê±°ì˜ ì—†ìŒ (pretrainì€ ì´ˆê¸°í™”ì¼ ë¿)

---

**ì‘ì„±ì¼**: 2025-10-16
**ë²„ì „**: 1.0
**ìƒíƒœ**: í˜„ì¬ êµ¬í˜„ (ì˜µì…˜ 2) ìœ ì§€ ê¶Œì¥
